---
title: "Simulation/Implementation of Honest Causal Forests"
subtitle: "R code for *Practical Guide to Honest Causal Forests for Identifying Heterogeneous Treatment Effects*"
author: "Authors: Neal Jawadekar, Katrina Kezios, Michelle C. Odden, Jeanette A. Stingone,  Sebastian Calonico, Kara E. Rudolph, Adina Zeki Al Hazzouri"
date: "November 2022"
output: html_document
---

### Overview
- The goal of this R program is to demonstrate an application of the honest causal forest, while also showing how the honest causal forest performs in simulated  
environments across varying levels of sample size, number of trees in an honest causal forest, differing correlations between covariates,
and varying levels of heterogeneities of C.A.T.E.s. 
- To pursue this goal, we generate two distinct settings: a RCT setting and observational setting. Within each setting, we run six different combinations of sample size/number of trees. 
in addition, we conduct all of these simulations across two different types of datasets based on realistic settings: one in which there is 
high correlation between covariates and small differences between C.A.T.E.s, and another in which there is low correlation between covariates and large differences 
between C.A.T.E.s. Furthermore, we run all observational simulations across two different types of doubly robust estimators (adjusted and unadjusted), thereby yielding a total of 
24 simulation scenarios in an observational setting and 12 simulation scenarios in an RCT setting. 
- For each simulation scenario, we run 1000 simulations (24 + 12)*1000 = 36000 total simulations
- Each of the 36000 simulated datasets is comprised of a dichotomous treatment assignment (A), 20 covariates (vector X, comprised of variables B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U), and a dichotomous outcome (Y), with
no individuals lost to follow-up. We also ensure that some of the 20 additional covariates that we simulate (B and N) 
are effect modifiers, such that the conditional probability of Y, given A, varies across levels of these effect modifiers. For our observational simulation, we include a confounder (variable D).
- Upon running this code, detailed results of the simulations can be found in the "sim_results#" tables. Model performance of the honest causal forests, averaged 
across each group of 1000 simulations, can be found in the summarize# tables.
- Model performance is assessed based on (a) the correct identification of effect modifiers, and (b) accurate estimation of the C.A.T.E.s defined within strata of 
the pre-specified effect modifiers.

**Please read our Appendix for the step-by-step instructions associated with this code.**

# NOTE 
# If running these simulations in RStudio using AWS EC2 cluster, you will need to first sequentially run these commands in the terminal to ensure the "faux" library can be installed correctly
# wget https://github.com/Kitware/CMake/releases/download/v3.20.2/cmake-3.20.2.tar.gz
# tar -zxvf cmake-3.20.2.tar.gz
# cd cmake-3.20.2
# ./bootstrap
# make 
# sudo make install                    
# then, you'll need to enter instance_id as the password
# Then, run this normally: install.packages("faux")



### Create sim_cf_rct function for randomized setting
```{r func, include=TRUE}
# Create function that inputs 6 parameters (nIndividuals, nTrees, correlation, cates, nSims, and combo), and uses them to simulate nSims RCT datasets.
sim_cf_rct <- function(nIndividuals, nTrees, correlation, cates, nSims, combo) {
  for(i in 1:nSims){ 
  # Step 1: First, we set conditional logic to say that if we are on the first simulation, then re-establish the following variables as empty vectors:
  if(i == 1) {
         trialnum <- numeric(nSims) # creates an empty vector 
         
         # Creating placeholders for our VIF calculations (which we will use to later summarize our results):
         B_position = numeric(nSims)
         C_position = numeric(nSims)
         D_position = numeric(nSims)
         E_position = numeric(nSims)
         F_position = numeric(nSims)
         G_position = numeric(nSims)
         H_position = numeric(nSims)
         I_position = numeric(nSims)
         J_position = numeric(nSims)
         K_position = numeric(nSims)
         L_position = numeric(nSims)
         M_position = numeric(nSims)
         N_position = numeric(nSims)
         O_position = numeric(nSims)
         P_position = numeric(nSims)
         Q_position = numeric(nSims)
         R_position = numeric(nSims)
         S_position = numeric(nSims)
         T_position = numeric(nSims)
         U_position = numeric(nSims)
         
         # Creating placeholders for our stratum-specific C.A.T.E.s and 95% CIs (which we will use to later summarize our results):
         strata1_CATE = numeric(nSims)
         strata1_LCL = numeric(nSims)
         strata1_UCL = numeric(nSims)       
         strata2_CATE = numeric(nSims)
         strata2_LCL = numeric(nSims)
         strata2_UCL = numeric(nSims)
         strata3_CATE = numeric(nSims)
         strata3_LCL = numeric(nSims)
         strata3_UCL = numeric(nSims)
         strata4_CATE = numeric(nSims)
         strata4_LCL = numeric(nSims)
         strata4_UCL = numeric(nSims)
         
         num_tiles = 4
         covariate_names = c('B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U')
  }
   
  # Step 2: Next, we simulate the individual variables in each dataset
  pid=seq(1, by=1, len=nIndividuals) # this creates a sequential list of "pid" from 1 to nIndividuals   
  A =rbinom(n = nIndividuals, size = 1, prob = 0.5)  # creates a randomly allocated treatment variable, A (each individual has a 50% probability of treatment)
          
  # Below, we simulate 20 covariates that are unassociated with the treatment, as would be the case in an RCT following randomization; we mimic this in the 
  # simulated data by randomly generating values of these variables. When we later define the outcome vector (Y), we also model some of these covariates (B and N) as effect
  # modifiers. Lastly, we acknowledge that in most randomized settings, some of these covariates (B/C and N/O, respectively) may be correlated with each other (although not with the exposure).    
  # We set their correlations depending on an input variable for our sim_cf_rct function, “correlation.”  
  if(correlation == "high") { 
         dat1 <-   rnorm_multi(n = nIndividuals, mu = c(20,23), sd = c(6,9),r = c(0.45), varnames = c("B_cont","C_cont"), empirical = FALSE) 
         dat2 <-   rnorm_multi(n = nIndividuals, mu = c(25,20), sd = c(14,11),r = c(0.75), varnames = c("N","O"), empirical = FALSE) 
  }        
  if(correlation == "low") {
         dat1 <-   rnorm_multi(n = nIndividuals, mu = c(20,23), sd = c(6,9),r = c(0.05), varnames = c("B_cont","C_cont"), empirical = FALSE) 
         dat2 <-   rnorm_multi(n = nIndividuals, mu = c(25,20), sd = c(14,11),r = c(0.15), varnames = c("N","O"), empirical = FALSE)
  }
  
  # Simulate dichotomous variables (B through K)	 
  B = ifelse(dat1$B_cont > median(dat1$B_cont),1,0)
  C = ifelse(dat1$C_cont > median(dat1$C_cont),1,0)
  chisq.test(B, C, correct=FALSE) # This test shows that B and C are associated        
  D = rbinom(n = nIndividuals, size = 1, prob = 0.3)
  E = rbinom(n = nIndividuals, size = 1, prob = 0.7)
  F = rbinom(n = nIndividuals, size = 1, prob = 0.13)
  G = rbinom(n = nIndividuals, size = 1, prob = 0.25)
  H = rbinom(n = nIndividuals, size = 1, prob = 0.30)
  I = rbinom(n = nIndividuals, size = 1, prob = 0.08)
  J = rbinom(n = nIndividuals, size = 1, prob = 0.15)
  K = rbinom(n = nIndividuals, size = 1, prob = 0.25)
          
  # Simulate continuous variables (L through U)
  L = rnorm(n = nIndividuals, mean = 30, sd = 12)
  M = rnorm(n = nIndividuals, mean = 15, sd = 0.1) 
  # Create 2 correlated variables, N and O
  N = dat2$N
  O = dat2$O
  cor(dat2, method = "pearson")[1,2] # This test shows that N and O are correlated
  P = rnorm(n = nIndividuals, mean = 120, sd = 4)
  Q = rnorm(n = nIndividuals, mean = 72, sd = 2.5)
  R = rnorm(n = nIndividuals, mean = 5, sd = 0.5)
  S = rnorm(n = nIndividuals, mean = 22, sd = 2)
  T = rnorm(n = nIndividuals, mean = 50, sd = 3)
  U = rnorm(n = nIndividuals, mean = 100, sd = 10)
        
  # Step 3: Next, we specify the conditional probabilities of Y within specific strata of treatment/covariate combinations. By specifying these conditional probabilities,
  # we are generating effect modification (i.e., varying A-Y relationship across levels of certain covariates, B and N). 
  # This logic was inspired by RCT simulation code written by Andrew Althous, viewable at: https://github.com/aalthous/RCT-Simulation-v1/blob/main/RCT_Binary_Outcome.R
  Yprob <- numeric(nIndividuals) # this creates an empty vector which is used to assign probability of Y for each individual  
  # We are assigning probabilities of Y based on each individual's values of A, B, and N, as well as the sim_cf_rct function's input value of "cates"   
  
  if(cates == "small") {
         Yprob[A == 1 & B == 1 & N >=41]=0.01 
         Yprob[A == 0 & B == 1 & N >= 41]=0.06
         # when B == 1 and N >= 41, C.A.T.E. of A->Y should be about -0.05
             
         Yprob[A == 1 & B == 0 & N >=41 ]=0.01
         Yprob[A == 0 & B == 0 & N >= 41]=0.02
         # when B == 0 and N >= 41, C.A.T.E. of A->Y should be about -0.01
             
         Yprob[A == 1 & B == 0 & N < 41]=0.05
         Yprob[A == 0 & B == 0 & N < 41]=0.01
         # when B == 0 and N < 41, C.A.T.E. of A->Y should be about 0.04
             
         Yprob[A == 1 & B == 1 & N < 41]=0.04
         Yprob[A == 0 & B == 1 & N < 41]=0.04
         # when B == 1 and N < 41, C.A.T.E. of A->Y should be about 0.00
  }        
  
  if(cates == "large") {
         Yprob[A == 1 & B == 1 & N >=41]=0.05 
         Yprob[A == 0 & B == 1 & N >= 41]=0.20
         # when B == 1 and N >= 41, C.A.T.E. of A->Y should be about -0.15
             
         Yprob[A == 1 & B == 0 & N >=41 ]=0.07 
         Yprob[A == 0 & B == 0 & N >= 41]=0.10
         # when B == 0 and N >= 41, C.A.T.E. of A->Y should be about -0.03
             
         Yprob[A == 1 & B == 0 & N < 41]=0.15
         Yprob[A == 0 & B == 0 & N < 41]=0.10
         # when B == 0 and N < 41, C.A.T.E. of A->Y should be about 0.05
             
         Yprob[A == 1 & B == 1 & N < 41]=0.15
         Yprob[A == 0 & B == 1 & N < 41]=0.05
         # when B == 1 and N < 41, C.A.T.E. of A->Y should be about 0.10
  }        
  # Next, we simulate each individual's outcome as a random draw from the binomial distribution of probabilities assigned above  
  Y=rbinom(n = nIndividuals, size = 1, prob = Yprob) 
  
  # Step 4: Then, we can create a data frame which combines pid, A, Y, and all covariates
  trialdata=data.frame(cbind(pid, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, Y)) 
 
  # This simply tells us which simulation each row of our results came from (counting upward from 1 to nSims)
  trialnum[i]=i 

  # Delete individual data objects 
  rm(A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, Y) 
  rm(dat1, dat2)  

  library(grf) # load grf package
  
  ### Step 5: Run honest causal forest on simulated dataset
  # Documentation for the grf package is viewable at: https://cran.r-project.org/web/packages/grf/grf.pdf
  # Please note: by default, the causal_forest function will perform R-learner and the AIPW (through utilization of the Y.hat and W.hat arguments). If not explicitly specified, Y.hat and W.hat will be a function of covariates, X.
  X = data.matrix(trialdata[, c(3:22)])  # to replicate this paper's results, one can use:  X = model.matrix(~ ., data = trialdata[, 3:22])
  Y = trialdata$Y
  W = trialdata$A
  
  causalf <- causal_forest(
    X = X,
    Y = Y,
    W = W,
    num.trees = nTrees, 
    mtry = min(ceiling(sqrt(ncol(X)-1) + 20), ncol(X)-1), 
    honesty = TRUE,
    honesty.fraction = 0.5,
    honesty.prune.leaves = TRUE,
    seed = 112, 
    min.node.size = 5
  )        

  #  Step 6: Evaluate if the honest causal forest correctly identified the true effect modifiers (B and N), by assessing the variable importance factor ranking list 
  # (variables that contributed most to heterogeneity of effect)
  
  vif_table = causalf %>% 
    variable_importance(decay.exponent = 2, max.depth = 4) %>% 
    as.data.frame() %>% 
    mutate(variable = colnames(causalf$X.orig)) %>% 
    arrange(desc(V1)) %>% filter(variable != "(Intercept)") %>% mutate(rn = row_number())
  
  B_position[i] = vif_table  %>% filter(variable == "B") %>% select(-variable, -V1) 
  C_position[i] = vif_table  %>% filter(variable == "C") %>% select(-variable, -V1) 
  D_position[i] = vif_table  %>% filter(variable == "D") %>% select(-variable, -V1) 
  E_position[i] = vif_table  %>% filter(variable == "E") %>% select(-variable, -V1) 
  F_position[i] = vif_table  %>% filter(variable == "F") %>% select(-variable, -V1) 
  G_position[i] = vif_table  %>% filter(variable == "G") %>% select(-variable, -V1) 
  H_position[i] = vif_table  %>% filter(variable == "H") %>% select(-variable, -V1) 
  I_position[i] = vif_table  %>% filter(variable == "I") %>% select(-variable, -V1) 
  J_position[i] = vif_table  %>% filter(variable == "J") %>% select(-variable, -V1) 
  K_position[i] = vif_table  %>% filter(variable == "K") %>% select(-variable, -V1) 
  L_position[i] = vif_table  %>% filter(variable == "L") %>% select(-variable, -V1) 
  M_position[i] = vif_table  %>% filter(variable == "M") %>% select(-variable, -V1) 
  N_position[i] = vif_table  %>% filter(variable == "N") %>% select(-variable, -V1) 
  O_position[i] = vif_table  %>% filter(variable == "O") %>% select(-variable, -V1) 
  P_position[i] = vif_table  %>% filter(variable == "P") %>% select(-variable, -V1) 
  Q_position[i] = vif_table  %>% filter(variable == "Q") %>% select(-variable, -V1) 
  R_position[i] = vif_table  %>% filter(variable == "R") %>% select(-variable, -V1) 
  S_position[i] = vif_table  %>% filter(variable == "S") %>% select(-variable, -V1) 
  T_position[i] = vif_table  %>% filter(variable == "T") %>% select(-variable, -V1) 
  U_position[i] = vif_table  %>% filter(variable == "U") %>% select(-variable, -V1) 
  
  ###  Step 7:
  # Evaluate if the honest causal forest accurately estimated C.A.T.E.s within specific strata of B and N as specified in the simulation. Here, we utilize the average_treatment_effect function, which generates a doubly robust estimate of the CATE, using an AIPW estimator.
  
  strata1_CATE[i] = average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 1 & unlist(causalf$X[,c("N")]) >= 41))[1] 
  strata1_LCL[i] = strata1_CATE[i] - qnorm(0.975) * (average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 1 & unlist(causalf$X[,c("N")]) >= 41))[2]) 
  strata1_UCL[i] = strata1_CATE[i] + qnorm(0.975) * (average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 1 & unlist(causalf$X[,c("N")]) >= 41))[2]) 
  
  # STRATA 2: Select all observations in dataset where B = 0 and N >= 41 --> calculate the C.A.T.E., Lower 95% CI, and Upper 95% CI 
  strata2_CATE[i] = average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 0 & unlist(causalf$X[,c("N")]) >= 41))[1] 
  strata2_LCL[i] = strata2_CATE[i] - qnorm(0.975) * (average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 0 & unlist(causalf$X[,c("N")]) >= 41))[2]) 
  strata2_UCL[i] = strata2_CATE[i] + qnorm(0.975) * (average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 0 & unlist(causalf$X[,c("N")]) >= 41))[2])
  
  # STRATA 3: Select all observations in dataset where B = 0 and N < 41 --> calculate the C.A.T.E., Lower 95% CI, and Upper 95% CI  
  strata3_CATE[i] = average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 0 & unlist(causalf$X[,c("N")]) < 41))[1]
  strata3_LCL[i] = strata3_CATE[i] - qnorm(0.975) * (average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 0 & unlist(causalf$X[,c("N")]) < 41))[2]) 
  strata3_UCL[i] = strata3_CATE[i] + qnorm(0.975) * (average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 0 & unlist(causalf$X[,c("N")]) < 41))[2]) 
  
  # STRATA 4: Select all observations in dataset where B = 1 and N < 41 --> calculate the C.A.T.E., Lower 95% CI, and Upper 95% CI  
  strata4_CATE[i] = average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 1 & unlist(causalf$X[,c("N")]) < 41))[1]  
  strata4_LCL[i] = strata4_CATE[i] - qnorm(0.975) * (average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 1 & unlist(unlist(causalf$X[,c("N")])) < 41))[2]) 
  strata4_UCL[i] = strata4_CATE[i] + qnorm(0.975) * (average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 1 & unlist(unlist(causalf$X[,c("N")])) < 41))[2])
  # Calculate averages of covariates across quartiles of CATE (alternative way to identify potential effect modifiers)
  # Estimate predicted CATE for each person
  oob_pred = predict(causalf, estimate.variance=TRUE) 
  oob_tauhat_cf <- oob_pred$predictions

  # Bring CATE to trialdata and assign unique dataset name to trialdata
  trialdata$cate <- oob_tauhat_cf
  assign(paste0("trialdata", i), trialdata)
  rm(trialdata)
  
  }  # this brace closes the "for" loop that iterates over nSims
 
  # Step 8: Alternative method for assessing importance of variables: covariates averaged across quartiles
# The below code (for exporting table of averaged covariates by quartile) is borrowed from an online tutorial written by Susan Athey et al., https://gsbdbi.github.io/ml_tutorial/hte_tutorial/hte_tutorial.html
  
  # Combine trialdata's across all simulations 
all_trials =  do.call("rbind",mget(ls(pattern = "^triald.*")))
# Remove individual trialdata
rm(list=grep("triald",ls(),value=TRUE,invert=FALSE))

# Split all CATEs across all trials into quartiles, to enable summarization of covariates
all_trials$ntile <- factor(ntile(all_trials$cate, n=num_tiles))   
# Regress each covariate on leaf assignment to means p
cov_means <- lapply(covariate_names, function(covariate) {
  lm_robust(as.formula(paste0(covariate, ' ~ 0 + ntile')), data = all_trials)
})

# Extract the mean and standard deviation of each covariate per leaf
cov_table <- lapply(cov_means, function(cov_mean) {
  as.data.frame(t(coef(summary(cov_mean))[,c("Estimate", "Std. Error")]))
})

# Preparation to color the chart
temp_standardized <- sapply(seq_along(covariate_names), function(j) {
covariate_name <- covariate_names[j]
.mean <- mean(all_trials[, covariate_name], na.rm = TRUE)
.sd <- sd(all_trials[, covariate_name], na.rm = TRUE)
m <- as.matrix(round(signif(cov_table[[j]], digits=4), 3))
.standardized <- (m["Estimate",] - .mean) / .sd
})

color_scale <- max(abs(c(max(temp_standardized, na.rm = TRUE), min(temp_standardized, na.rm = TRUE))))
color_scale <- color_scale * c(-1,1)

# Little trick to display the standard errors
 table <- lapply(seq_along(covariate_names), function(j) { 
   covariate_name <- covariate_names[j] 
   .mean <- mean(all_trials[, covariate_name], na.rm = TRUE)
   .sd <- sd(all_trials[, covariate_name], na.rm = TRUE) 
   m <- as.matrix(round(signif(cov_table[[j]], digits=4), 3))
   .standardized <- (m["Estimate",] - .mean) / .sd 
   m["Estimate",] <- (cell_spec(m["Estimate",], 
                                color = "White", 
                                background = spec_color(.standardized,
              end = 0.9, 
              begin = 0.1,
              scale_from = color_scale)
                                ))
                       m["Std. Error",] <- paste0("(", m["Std. Error",], ")") 
   m
 }) 
 table <- do.call(rbind, table) 

 # Covariate names
 covnames <- rep("", nrow(table))
 covnames[seq(1, length(covnames), 2)] <-
  cell_spec(covariate_names, format = "html", escape = F, color = "black", bold = T) 
 
 
 table <- cbind(covariates=covnames, table)
 
 # Title of table
 caption <- paste0("Average covariate values in each n-tile: ", nIndividuals, " nIndividuals and ", nTrees, " nTrees,", correlation, " correlation, and ", cates, " cates")
  # Export table of covariates averaged within quartiles to your working directory
table %>%
  kable(format="html", digits=2, caption=caption, escape = FALSE, row.names = FALSE) %>%
  kable_styling(bootstrap_options=c("condensed", "responsive"), full_width=FALSE) %>%
  footnote(paste0("Colors are assigned according to where the subgroup's mean value lands on the standardized empirical distribution of it's variable: (x - mean(x))/sd(x)<br>Standardized distribution is colored from a scale of +/-", round(color_scale[2], 3)), escape = FALSE) %>% save_kable(paste0(".//Variables_by_Quartile_",combo,"_rct.jpeg"))
      
  # Step 9: Aggregate and save simulation results   
  # Combine simulation results, and save it as a temporary dataset
  sim_results = data.frame(cbind(trialnum, B_position,C_position,D_position,E_position,F_position,G_position,H_position,I_position,J_position,K_position,L_position,M_position,N_position,O_position,P_position,Q_position,R_position,S_position,T_position,U_position,strata1_CATE,strata1_LCL, strata1_UCL, strata2_CATE, strata2_LCL,strata2_UCL,strata3_CATE,strata3_LCL,strata3_UCL, strata4_CATE,strata4_LCL, strata4_UCL)  %>% 
		as.data.frame() %>% 
		mutate(nIndividuals = nIndividuals, nTrees = nTrees))
      
  # Save the data frame with a name that we'll use later
  assign(paste0("sim_results_", combo,"_rct"), data.frame(sim_results))
  
   # Write all simulation results to CSV in your working directory
  apply(sim_results,2,as.character) %>%   write.csv(paste0(".//sim_results_",combo,"_rct.csv"), row.names = FALSE)
  
  
# Summarize results (means) from this group of simulations
assign("summarize", data.frame(sim_results %>% 
select(-nIndividuals, -nTrees, -trialnum) %>% 
mutate(B_position = as.numeric(B_position), C_position = as.numeric(C_position),D_position = as.numeric(D_position), E_position = as.numeric(E_position),
 F_position = as.numeric(F_position), G_position = as.numeric(G_position),H_position = as.numeric(H_position), I_position = as.numeric(I_position), 
 J_position = as.numeric(J_position), K_position = as.numeric(K_position),L_position = as.numeric(L_position),M_position = as.numeric(M_position),
 N_position=as.numeric(N_position),O_position = as.numeric(O_position), P_position = as.numeric(P_position),Q_position = as.numeric(Q_position),
 R_position=as.numeric(R_position),S_position=as.numeric(S_position),T_position=as.numeric(T_position),U_position=as.numeric(U_position),
 strata1_CATE = as.numeric(strata1_CATE),strata1_LCL = as.numeric(strata1_LCL),strata1_UCL = as.numeric(strata1_UCL),
 strata2_CATE = as.numeric(strata2_CATE),strata2_LCL = as.numeric(strata2_LCL),strata2_UCL = as.numeric(strata2_UCL),
 strata3_CATE = as.numeric(strata3_CATE),strata3_LCL = as.numeric(strata3_LCL),strata3_UCL = as.numeric(strata3_UCL),
 strata4_CATE = as.numeric(strata4_CATE),strata4_LCL = as.numeric(strata4_LCL),strata4_UCL = as.numeric(strata4_UCL)) %>% 
 summarise_at(vars(B_position:strata4_UCL), mean)) %>% mutate(across(1:20, round, 3),across(21:32, round, 4)))


# Transform table from wide to long
pivoted_table = pivot_longer(summarize, cols = everything(), values_to = "Average_Value",names_to = "Variable")  %>% mutate(Average_Value = as.numeric(Average_Value))
# Order the table by "Average_Value"
ordered_table1 = pivoted_table %>% filter(substr(Variable,3,10) == "position")
ordered_table2 = ordered_table1[order(ordered_table1$Average_Value),]
cates_table = pivoted_table %>% filter(substr(Variable,3,10) != "position")
summary_table = rbind(ordered_table2, cates_table)

# Write summarized results to CSV in your working directory
  summary_table %>%   write.csv(paste0(".//summarize_",combo,"_rct.csv"), row.names = FALSE)
  # Save the data frame with a name that we'll use for dataframe
  assign(paste0("summarize_",combo,"_rct"), data.frame(summary_table))
    
# Clear all objects from R except for the final summarize and results tables
rm(list=grep("summarize|sim_result",ls(),value=TRUE,invert=TRUE))


# Return relevant data frames
if(exists("sim_results_1_rct")) {
   sim_results_1_rct <<- sim_results_1_rct
   summarize_1_rct <<- summarize_1_rct
}
if(exists("sim_results_2_rct")) {
   sim_results_2_rct <<- sim_results_2_rct
   summarize_2_rct <<- summarize_2_rct
}
if(exists("sim_results_3_rct")) {
   sim_results_3_rct <<- sim_results_3_rct
   summarize_3_rct <<- summarize_3_rct
}
if(exists("sim_results_4_rct")) {
   sim_results_4_rct <<- sim_results_4_rct
   summarize_4_rct <<- summarize_4_rct
}
if(exists("sim_results_5_rct")) {
   sim_results_5_rct <<- sim_results_5_rct
   summarize_5_rct <<- summarize_5_rct
}
if(exists("sim_results_6_rct")) {
   sim_results_6_rct <<- sim_results_6_rct
   summarize_6_rct <<- summarize_6_rct
}
if(exists("sim_results_7_rct")) {
   sim_results_7_rct <<- sim_results_7_rct
   summarize_7_rct <<- summarize_7_rct
}
if(exists("sim_results_8_rct")) {
   sim_results_8_rct <<- sim_results_8_rct
   summarize_8_rct <<- summarize_8_rct
}
if(exists("sim_results_9_rct")) {
   sim_results_9_rct <<- sim_results_9_rct
   summarize_9_rct <<- summarize_9_rct
}
if(exists("sim_results_10_rct")) {
   sim_results_10_rct <<- sim_results_10_rct
   summarize_10_rct <<- summarize_10_rct
}
if(exists("sim_results_11_rct")) {
   sim_results_11_rct <<- sim_results_11_rct
   summarize_11_rct <<- summarize_11_rct
}
if(exists("sim_results_12_rct")) {
   sim_results_12_rct <<- sim_results_12_rct
   summarize_12_rct <<- summarize_12_rct
}
} # end of the sim_cf_rct function
```

**Our sim_cf_rct function (above) has now been defined.**
**Next, in Code Chunks 1-6, we run RCT simulations using *high* correlations between variables and *small* heterogeneities in C.A.T.E.s.**

### Code Chunk 1: 1st group of simulations
```{r sim1, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# correlation can take the value of “high” or “low”, whereas cates can take the value of “small” or “large”
# combo can be any integer ranging from 1 to 12
# Run the 1st group of simulations, using 1000 individuals and 200 trees
sim_cf_rct(nIndividuals = 1000, nTrees = 200, correlation = "high", cates = "small", nSims = 1000, combo = 1)
```

### Code Chunk 2: 2nd group of simulations
```{r sim2, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 2nd group of simulations, using 1000 individuals and 2000 trees
sim_cf_rct(nIndividuals = 1000, nTrees = 2000, correlation = "high", cates = "small", nSims = 1000, combo = 2)
```

### Code Chunk 3: 3rd group of simulations 
```{r sim3, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 3rd group of simulations, using 10000 individuals and 200 trees
sim_cf_rct(nIndividuals = 10000, nTrees = 200, correlation = "high", cates = "small", nSims = 1000, combo = 3)
```

### Code Chunk 4: 4th group of simulations 
```{r sim4, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 4th group of simulations, using 10000 individuals and 2000 trees
sim_cf_rct(nIndividuals = 10000, nTrees = 2000, correlation = "high", cates = "small", nSims = 1000, combo = 4)
```

### Code Chunk 5: 5th group of simulations 
```{r sim5, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 5th group of simulations, using 40000 individuals and 200 trees
sim_cf_rct(nIndividuals = 40000, nTrees = 200, correlation = "high", cates = "small", nSims = 1000, combo = 5)
```

### Code Chunk 6: 6th group of simulations 
```{r sim6, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 6th group of simulations, using 40000 individuals and 2000 trees
sim_cf_rct(nIndividuals = 40000, nTrees = 2000, correlation = "high", cates = "small", nSims = 1000, combo = 6)
```


**In Code Chunks 7-12, we run RCT simulations using *low* correlations between variables and *large* heterogeneities in C.A.T.E.s.**


### Code Chunk 7: 7th group of simulations
```{r sim7, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 7th group of simulations, using 1000 individuals and 200 trees
sim_cf_rct(nIndividuals = 1000, nTrees = 200, correlation = "low", cates = "large", nSims = 1000, combo = 7)
```

### Code Chunk 8: 8th group of simulations
```{r sim8, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 8th group of simulations, using 1000 individuals and 2000 trees
sim_cf_rct(nIndividuals = 1000, nTrees = 2000, correlation = "low", cates = "large", nSims = 1000, combo = 8)
```

### Code Chunk 9: 9th group of simulations 
```{r sim9, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 9th group of simulations, using 10000 individuals and 200 trees
sim_cf_rct(nIndividuals = 10000, nTrees = 200, correlation = "low", cates = "large", nSims = 1000, combo = 9)
```

### Code Chunk 10: 10th group of simulations 
```{r sim10, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 10th group of simulations, using 10000 individuals and 2000 trees
sim_cf_rct(nIndividuals = 10000, nTrees = 2000, correlation = "low", cates = "large",  nSims = 1000, combo = 10)
```

### Code Chunk 11: 11th group of simulations 
```{r sim11, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 11th group of simulations, using 40000 individuals and 200 trees
sim_cf_rct(nIndividuals = 40000, nTrees = 200, correlation = "low", cates = "large",  nSims = 1000, combo = 11)
```

### Code Chunk 12: 12th group of simulations 
```{r sim12, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 12th group of simulations, using 40000 individuals and 2000 trees
sim_cf_rct(nIndividuals = 40000, nTrees = 2000, correlation = "low", cates = "large",  nSims = 1000, combo = 12)
```



### Create sim_cf_obs function for observational setting
```{r func, include=TRUE}
# Create function that inputs 7 parameters (nIndividuals, nTrees, correlation, cates, nSims, combo, and ortho), and uses them to simulate nSims observational datasets.
sim_cf_obs <- function(nIndividuals, nTrees, correlation, cates, nSims, combo, ortho) {
  for(i in 1:nSims){ 
  # Step 1: First, we set conditional logic to say that if we are on the first simulation, then re-establish the following variables as empty vectors:
  if(i == 1) {
   
         trialnum <- numeric(nSims) # creates an empty vector 
         
         # Creating placeholders for our VIF calculations (which we will use to later summarize our results):
         B_position = numeric(nSims)
         C_position = numeric(nSims)
         D_position = numeric(nSims)
         E_position = numeric(nSims)
         F_position = numeric(nSims)
         G_position = numeric(nSims)
         H_position = numeric(nSims)
         I_position = numeric(nSims)
         J_position = numeric(nSims)
         K_position = numeric(nSims)
         L_position = numeric(nSims)
         M_position = numeric(nSims)
         N_position = numeric(nSims)
         O_position = numeric(nSims)
         P_position = numeric(nSims)
         Q_position = numeric(nSims)
         R_position = numeric(nSims)
         S_position = numeric(nSims)
         T_position = numeric(nSims)
         U_position = numeric(nSims)
         
         # Creating placeholders for our stratum-specific C.A.T.E.s and 95% CIs (which we will use to later summarize our results):
         strata1_CATE = numeric(nSims)
         strata1_LCL = numeric(nSims)
         strata1_UCL = numeric(nSims)       
         strata2_CATE = numeric(nSims)
         strata2_LCL = numeric(nSims)
         strata2_UCL = numeric(nSims)
         strata3_CATE = numeric(nSims)
         strata3_LCL = numeric(nSims)
         strata3_UCL = numeric(nSims)
         strata4_CATE = numeric(nSims)
         strata4_LCL = numeric(nSims)
         strata4_UCL = numeric(nSims)
         
         num_tiles = 4
         covariate_names = c('B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U')
  }
   
 # Step 2: Next, we simulate the individual variables in each dataset
  pid=seq(1, by=1, len=nIndividuals) # this creates a sequential list of "pid" from 1 to nIndividuals   
 
  # Below, we create 2 correlated variables, treatment (A) and confounder (D)
  corr <-   rnorm_multi(n = nIndividuals, mu = c(14,10), sd = c(3,2),r = c(0.18), varnames = c("A_cont","D_cont"), empirical = FALSE) 
  
  A = ifelse(corr$A_cont > as.numeric(data.frame(quantile(corr$A_cont, probs = c(0.7)))),1,0) # Creating treatment such that approximately 30% will be treated
  D = ifelse(corr$D_cont > median(corr$D_cont),1,0)
  chisq.test(A, D, correct=FALSE) # This test shows that A and D are associated 
  
  
  # Below, we simulate the remainder of the 20 covariates, some of which are correlated with other variables.
  # When we later define the outcome vector (Y), we also model some of these covariates (B and N) as effect
  # modifiers. Lastly, we acknowledge that some of these covariates (B/C and N/O, respectively) may be correlated with each other (although not with the exposure).    
  # We set their correlations depending on an input variable for our sim_cf_rct function, “correlation.” 
  if(correlation == "high") {
         dat1 <-   rnorm_multi(n = nIndividuals, mu = c(20,23), sd = c(6,9),r = c(0.45), varnames = c("B_cont","C_cont"), empirical = FALSE) 
         dat2 <-   rnorm_multi(n = nIndividuals, mu = c(25,20), sd = c(14,11),r = c(0.75), varnames = c("N","O"), empirical = FALSE) 
  }     
  if(correlation == "low") {
         dat1 <-   rnorm_multi(n = nIndividuals, mu = c(20,23), sd = c(6,9),r = c(0.05), varnames = c("B_cont","C_cont"), empirical = FALSE) 
         dat2 <-   rnorm_multi(n = nIndividuals, mu = c(25,20), sd = c(14,11),r = c(0.15), varnames = c("N","O"), empirical = FALSE)
  }
  
  # Simulate dichotomous variables (B through K)	 
  B = ifelse(dat1$B_cont > median(dat1$B_cont),1,0)
  C = ifelse(dat1$C_cont > median(dat1$C_cont),1,0)
  chisq.test(B, C, correct=FALSE) # This test shows that B and C are associated  
  E = rbinom(n = nIndividuals, size = 1, prob = 0.7)
  F = rbinom(n = nIndividuals, size = 1, prob = 0.13)
  G = rbinom(n = nIndividuals, size = 1, prob = 0.25) 
  H = rbinom(n = nIndividuals, size = 1, prob = 0.30)
  I = rbinom(n = nIndividuals, size = 1, prob = 0.08)
  J = rbinom(n = nIndividuals, size = 1, prob = 0.15)
  K = rbinom(n = nIndividuals, size = 1, prob = 0.25)
          
  # Simulate continuous variables (L through U)
  L = rnorm(n = nIndividuals, mean = 30, sd = 12)
  M = rnorm(n = nIndividuals, mean = 15, sd = 0.1) 
  # Create 2 correlated variables, N and O
  N = dat2$N
  O = dat2$O
  cor(dat2, method = "pearson")[1,2] # This test shows that N and O are correlated
  P = rnorm(n = nIndividuals, mean = 120, sd = 4)
  Q = rnorm(n = nIndividuals, mean = 72, sd = 2.5)  
  R = rnorm(n = nIndividuals, mean = 5, sd = 0.5)
  S = rnorm(n = nIndividuals, mean = 22, sd = 2)
  T = rnorm(n = nIndividuals, mean = 50, sd = 3)
  U = rnorm(n = nIndividuals, mean = 100, sd = 10)
  
 # Step 3: Next, we specify the conditional probabilities of Y within specific strata of treatment/covariate combinations. By specifying these conditional probabilities,
  # we are generating effect modification (i.e., varying A-Y relationship across levels of certain covariates, B and N), as well as a confounder, D. 
  # This logic was inspired by simulation code written by Andrew Althous, viewable at: https://github.com/aalthous/RCT-Simulation-v1/blob/main/RCT_Binary_Outcome.R
  Yprob <- numeric(nIndividuals) # this creates an empty vector which is used to assign probability of Y for each individual         
  # We are assigning probabilities of Y based on each individual's values of A, B, and N, as well as the sim_cf_obs function's input value of "cates"   
       
  if(cates == "small") {
         Yprob[A == 1 & B == 1 & N >=41 & D == 1]=0.01
         Yprob[A == 0 & B == 1 & N >= 41 & D == 1]=0.06
         Yprob[A == 1 & B == 1 & N >=41 & D == 0]=0.09
         Yprob[A == 0 & B == 1 & N >= 41 & D == 0]=0.14
         # when B == 1 and N >= 41, C.A.T.E. of A->Y should be about -0.05
             
         Yprob[A == 1 & B == 0 & N >=41 & D == 1]=0.01
         Yprob[A == 0 & B == 0 & N >= 41 & D == 1]=0.02
         Yprob[A == 1 & B == 0 & N >=41 & D == 0]=0.08
         Yprob[A == 0 & B == 0 & N >= 41 & D == 0]=0.09
         # when B == 0 and N >= 41, C.A.T.E. of A->Y should be about -0.01
             
         Yprob[A == 1 & B == 0 & N < 41 & D == 1]=0.05
         Yprob[A == 0 & B == 0 & N < 41 & D == 1]=0.01
         Yprob[A == 1 & B == 0 & N < 41 & D == 0]=0.13
         Yprob[A == 0 & B == 0 & N < 41 & D == 0]=0.09
         # when B == 0 and N < 41, C.A.T.E. of A->Y should be about 0.04
             
         Yprob[A == 1 & B == 1 & N < 41 & D == 1]=0.04
         Yprob[A == 0 & B == 1 & N < 41 & D == 1]=0.04
         Yprob[A == 1 & B == 1 & N < 41 & D == 0]=0.11
         Yprob[A == 0 & B == 1 & N < 41 & D == 0]=0.11
         # when B == 1 and N < 41, C.A.T.E. of A->Y should be about 0.00
    }        
  
  if(cates == "large") {
         Yprob[A == 1 & B == 1 & N >=41 & D == 1]=0.05 
         Yprob[A == 0 & B == 1 & N >= 41 & D == 1]=0.20
         Yprob[A == 1 & B == 1 & N >=41 & D == 0]=0.11 
         Yprob[A == 0 & B == 1 & N >= 41 & D == 0]=0.26
         # when B == 1 and N >= 41, C.A.T.E. of A->Y should be about -0.15
             
         Yprob[A == 1 & B == 0 & N >=41 & D == 1]=0.07 
         Yprob[A == 0 & B == 0 & N >= 41 & D == 1]=0.10
         Yprob[A == 1 & B == 0 & N >=41 & D == 0]=0.14 
         Yprob[A == 0 & B == 0 & N >= 41 & D == 0]=0.17
         # when B == 0 and N >= 41, C.A.T.E. of A->Y should be about -0.03
             
         Yprob[A == 1 & B == 0 & N < 41 & D == 1]=0.07
         Yprob[A == 0 & B == 0 & N < 41 & D == 1]=0.02
         Yprob[A == 1 & B == 0 & N < 41 & D == 0]=0.15
         Yprob[A == 0 & B == 0 & N < 41 & D == 0]=0.10
         # when B == 0 and N < 41, C.A.T.E. of A->Y should be about 0.05
             
         Yprob[A == 1 & B == 1 & N < 41 & D == 1]=0.13
         Yprob[A == 0 & B == 1 & N < 41 & D == 1]=0.03
         Yprob[A == 1 & B == 1 & N < 41 & D == 0]=0.20
         Yprob[A == 0 & B == 1 & N < 41 & D == 0]=0.10
         # when B == 1 and N < 41, C.A.T.E. of A->Y should be about 0.10
  }  
  # Next, we simulate each individual's outcome as a random draw from the binomial distribution of probabilities assigned above  
  Y=rbinom(n = nIndividuals, size = 1, prob = Yprob) 
  
  # Step 4: Then, we can create a data frame which combines pid, A, Y, and all covariates
  trialdata=data.frame(cbind(pid, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, Y)) 
  
  # This simply tells us which simulation each row of our results came from (counting upward from 1 to nSims)
  trialnum[i]=i 

  # Delete individual data objects 
  rm(A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, Y) 
  rm(dat1, dat2)  

  library(grf) # load grf package
  
  ### Step 5: Run honest causal forest on simulated dataset
  # Documentation for the grf package is viewable at: https://cran.r-project.org/web/packages/grf/grf.pdf
  # Please note: by default, the causal_forest function will perform R-learner and the AIPW (through utilization of the Y.hat and W.hat arguments). If not explicitly specified, Y.hat and W.hat will be a function of covariates, X (see when ortho == 1).
  X = data.matrix(trialdata[, c(3:22)])  # to replicate this paper's results, one can use:  X = model.matrix(~ ., data = trialdata[, 3:22])
  Y = trialdata$Y
  W = trialdata$A
 
  # If orthogonal, use the defaults for Y.hat and W.hat
   if(ortho == 1) {
   causalf <- causal_forest(
    X = X,
    Y = Y,
    W = W,
    num.trees = nTrees, 
    mtry = min(ceiling(sqrt(ncol(X)-1) + 20), ncol(X)-1), 
    honesty = TRUE,
    honesty.fraction = 0.5,
    honesty.prune.leaves = TRUE,
    seed = 112, 
    min.node.size = 5
  )  
  }
  
  # If non-orthogonal (i.e., unadjusted model), set Y.hat and W.hat to the mean
   if(ortho == 0) {
    causalf <- causal_forest(
    X = X,
    Y = Y,
    W = W,
    Y.hat =  mean(trialdata$Y), 
    W.hat = mean(trialdata$A), 
    num.trees = nTrees, 
    mtry = min(ceiling(sqrt(ncol(X)-1) + 20), ncol(X)-1), 
    honesty = TRUE,
    honesty.fraction = 0.5,
    honesty.prune.leaves = TRUE,
    seed = 112, 
    min.node.size = 5
  )        
  }

  # Step 6: Evaluate if the honest causal forest correctly identified the true effect modifiers (B and N), by assessing the variable importance factor ranking list 
  # (variables that contributed most to heterogeneity of effect)
  
  vif_table = causalf %>% 
    variable_importance(decay.exponent = 2, max.depth = 4) %>% 
    as.data.frame() %>% 
    mutate(variable = colnames(causalf$X.orig)) %>% 
    arrange(desc(V1)) %>% filter(variable != "(Intercept)") %>% mutate(rn = row_number())
  
  B_position[i] = vif_table  %>% filter(variable == "B") %>% select(-variable, -V1) 
  C_position[i] = vif_table  %>% filter(variable == "C") %>% select(-variable, -V1) 
  D_position[i] = vif_table  %>% filter(variable == "D") %>% select(-variable, -V1) 
  E_position[i] = vif_table  %>% filter(variable == "E") %>% select(-variable, -V1) 
  F_position[i] = vif_table  %>% filter(variable == "F") %>% select(-variable, -V1) 
  G_position[i] = vif_table  %>% filter(variable == "G") %>% select(-variable, -V1) 
  H_position[i] = vif_table  %>% filter(variable == "H") %>% select(-variable, -V1) 
  I_position[i] = vif_table  %>% filter(variable == "I") %>% select(-variable, -V1) 
  J_position[i] = vif_table  %>% filter(variable == "J") %>% select(-variable, -V1) 
  K_position[i] = vif_table  %>% filter(variable == "K") %>% select(-variable, -V1) 
  L_position[i] = vif_table  %>% filter(variable == "L") %>% select(-variable, -V1) 
  M_position[i] = vif_table  %>% filter(variable == "M") %>% select(-variable, -V1) 
  N_position[i] = vif_table  %>% filter(variable == "N") %>% select(-variable, -V1) 
  O_position[i] = vif_table  %>% filter(variable == "O") %>% select(-variable, -V1) 
  P_position[i] = vif_table  %>% filter(variable == "P") %>% select(-variable, -V1) 
  Q_position[i] = vif_table  %>% filter(variable == "Q") %>% select(-variable, -V1) 
  R_position[i] = vif_table  %>% filter(variable == "R") %>% select(-variable, -V1) 
  S_position[i] = vif_table  %>% filter(variable == "S") %>% select(-variable, -V1) 
  T_position[i] = vif_table  %>% filter(variable == "T") %>% select(-variable, -V1) 
  U_position[i] = vif_table  %>% filter(variable == "U") %>% select(-variable, -V1) 
  
  ###  Step 7:
  #  Evaluate if the honest causal forest accurately estimated C.A.T.E.s within specific strata of B and N as specified in the simulation. Here, we utilize the average_treatment_effect function, which generates a doubly robust estimate of the CATE, using an AIPW estimator.
  
  # STRATA 1: Select all observations in dataset where B = 1 and N >= 41 --> calculate the C.A.T.E., Lower 95% CI, and Upper 95% CI
  strata1_CATE[i] = average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 1 & unlist(causalf$X[,c("N")]) >= 41))[1] 
  strata1_LCL[i] = strata1_CATE[i] - qnorm(0.975) * (average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 1 & unlist(causalf$X[,c("N")]) >= 41))[2]) 
  strata1_UCL[i] = strata1_CATE[i] + qnorm(0.975) * (average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 1 & unlist(causalf$X[,c("N")]) >= 41))[2]) 
  
  # STRATA 2: Select all observations in dataset where B = 0 and N >= 41 --> calculate the C.A.T.E., Lower 95% CI, and Upper 95% CI 
  strata2_CATE[i] = average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 0 & unlist(causalf$X[,c("N")]) >= 41))[1] 
  strata2_LCL[i] = strata2_CATE[i] - qnorm(0.975) * (average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 0 & unlist(causalf$X[,c("N")]) >= 41))[2]) 
  strata2_UCL[i] = strata2_CATE[i] + qnorm(0.975) * (average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 0 & unlist(causalf$X[,c("N")]) >= 41))[2])
  
  # STRATA 3: Select all observations in dataset where B = 0 and N < 41 --> calculate the C.A.T.E., Lower 95% CI, and Upper 95% CI  
  strata3_CATE[i] = average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 0 & unlist(causalf$X[,c("N")]) < 41))[1]
  strata3_LCL[i] = strata3_CATE[i] - qnorm(0.975) * (average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 0 & unlist(causalf$X[,c("N")]) < 41))[2]) 
  strata3_UCL[i] = strata3_CATE[i] + qnorm(0.975) * (average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 0 & unlist(causalf$X[,c("N")]) < 41))[2]) 
  
  # STRATA 4: Select all observations in dataset where B = 1 and N < 41 --> calculate the C.A.T.E., Lower 95% CI, and Upper 95% CI  
  strata4_CATE[i] = average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 1 & unlist(causalf$X[,c("N")]) < 41))[1]  
  strata4_LCL[i] = strata4_CATE[i] - qnorm(0.975) * (average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 1 & unlist(unlist(causalf$X[,c("N")])) < 41))[2]) 
  strata4_UCL[i] = strata4_CATE[i] + qnorm(0.975) * (average_treatment_effect(causalf, target.sample = "all", subset = (unlist(causalf$X[,c("B")]) == 1 & unlist(unlist(causalf$X[,c("N")])) < 41))[2]) 
  
  
  # Calculate averages of covariates across quartiles of CATE (alternative way to identify potential effect modifiers)
  # Estimate predicted CATE for each person
  oob_pred = predict(causalf, estimate.variance=TRUE) 
  oob_tauhat_cf <- oob_pred$predictions

  # Bring CATE to trialdata and assign unique dataset name to trialdata
  trialdata$cate <- oob_tauhat_cf
  assign(paste0("trialdata", i), trialdata)
  rm(trialdata)
  
  }  # this brace closes the "for" loop that iterates over nSims
  
  # Step 8: Alternative method for assessing importance of variables: covariates averaged across quartiles
# The below code (for exporting table of averaged covariates by quartile) is borrowed from an online tutorial written by Susan Athey et al., https://gsbdbi.github.io/ml_tutorial/hte_tutorial/hte_tutorial.html

  # Combine trialdata's across all simulations 
all_trials =  do.call("rbind",mget(ls(pattern = "^triald.*")))
# Remove individual trialdata
rm(list=grep("triald",ls(),value=TRUE,invert=FALSE))
 
# Split all CATEs across all trials into quartiles, to enable summarization of covariates
all_trials$ntile <- factor(ntile(all_trials$cate, n=num_tiles))  
# Regress each covariate on leaf assignment to means p
cov_means <- lapply(covariate_names, function(covariate) {
  lm_robust(as.formula(paste0(covariate, ' ~ 0 + ntile')), data = all_trials)
})

# Extract the mean and standard deviation of each covariate per leaf
cov_table <- lapply(cov_means, function(cov_mean) {
  as.data.frame(t(coef(summary(cov_mean))[,c("Estimate", "Std. Error")]))
})

# Preparation to color the chart
temp_standardized <- sapply(seq_along(covariate_names), function(j) {
covariate_name <- covariate_names[j]
.mean <- mean(all_trials[, covariate_name], na.rm = TRUE)
.sd <- sd(all_trials[, covariate_name], na.rm = TRUE)
m <- as.matrix(round(signif(cov_table[[j]], digits=4), 3))
.standardized <- (m["Estimate",] - .mean) / .sd
})

color_scale <- max(abs(c(max(temp_standardized, na.rm = TRUE), min(temp_standardized, na.rm = TRUE))))
color_scale <- color_scale * c(-1,1)

# Little trick to display the standard errors
 table <- lapply(seq_along(covariate_names), function(j) { 
   covariate_name <- covariate_names[j] 
   .mean <- mean(all_trials[, covariate_name], na.rm = TRUE)
   .sd <- sd(all_trials[, covariate_name], na.rm = TRUE) 
   m <- as.matrix(round(signif(cov_table[[j]], digits=4), 3))
   .standardized <- (m["Estimate",] - .mean) / .sd 
   m["Estimate",] <- (cell_spec(m["Estimate",], 
                                color = "White", 
                                background = spec_color(.standardized,
              end = 0.9, 
              begin = 0.1,
              scale_from = color_scale)
                                ))
                       m["Std. Error",] <- paste0("(", m["Std. Error",], ")") 
   m
 }) 
 table <- do.call(rbind, table) 

 # Covariate names
 covnames <- rep("", nrow(table))
 covnames[seq(1, length(covnames), 2)] <-
  cell_spec(covariate_names, format = "html", escape = F, color = "black", bold = T) 
 
 
 table <- cbind(covariates=covnames, table)
 
 # Title of table
 if(ortho == 1) {
  caption <- paste0("Average covariate values in each n-tile: Observational Setting with covariate-adjusted doubly robust estimator - ", nIndividuals, " nIndividuals, ", nTrees, " nTrees, ", correlation, " correlation, and ", cates, " cates")
 }
 
  if(ortho == 0) {
  caption <- paste0("Average covariate values in each n-tile: Observational Setting with unadjusted doubly robust estimator - ", nIndividuals, " nIndividuals, ", nTrees, " nTrees, ", correlation, " correlation, and ", cates, " cates")
 }
  
  # Export table of covariates averaged within quartiles to your working directory
table %>%
  kable(format="html", digits=2, caption=caption, escape = FALSE, row.names = FALSE) %>%
  kable_styling(bootstrap_options=c("condensed", "responsive"), full_width=FALSE) %>%
  footnote(paste0("Colors are assigned according to where the subgroup's mean value lands on the standardized empirical distribution of it's variable: (x - mean(x))/sd(x)<br>Standardized distribution is colored from a scale of +/-", round(color_scale[2], 3)), escape = FALSE) %>% save_kable(paste0(".//Variables_by_Quartile_",combo,"_",ortho,".jpeg"))
  
  # Step 9: Aggregate and save simulation results
  # Combine simulation results, and save it as a temporary dataset
  sim_results = data.frame(cbind(trialnum, B_position,C_position,D_position,E_position,F_position,G_position,H_position,I_position,J_position,K_position,L_position,M_position,N_position,O_position,P_position,Q_position,R_position,S_position,T_position,U_position,strata1_CATE,strata1_LCL, strata1_UCL, strata2_CATE, strata2_LCL,strata2_UCL,strata3_CATE,strata3_LCL,strata3_UCL, strata4_CATE,strata4_LCL, strata4_UCL)  %>% 
		as.data.frame() %>% 
		mutate(nIndividuals = nIndividuals, nTrees = nTrees))
      
   # Save the data frame with a name that we'll use later
  assign(paste0("sim_results_", combo, "_", ortho), data.frame(sim_results))
  
  # Write all simulation results to CSV in your working directory
  apply(sim_results,2,as.character) %>%   write.csv(paste0(".//sim_results_",combo,"_",ortho,".csv"), row.names = FALSE)
  
  
# Summarize results (means) from this group of simulations
assign("summarize", data.frame(sim_results %>% 
 select(-nIndividuals, -nTrees, -trialnum) %>% 
 mutate(B_position = as.numeric(B_position), C_position = as.numeric(C_position),D_position = as.numeric(D_position), E_position = as.numeric(E_position),
 F_position = as.numeric(F_position), G_position = as.numeric(G_position),H_position = as.numeric(H_position), I_position = as.numeric(I_position), 
 J_position = as.numeric(J_position), K_position = as.numeric(K_position),L_position = as.numeric(L_position),M_position = as.numeric(M_position),
 N_position=as.numeric(N_position),O_position = as.numeric(O_position), P_position = as.numeric(P_position),Q_position = as.numeric(Q_position),
 R_position=as.numeric(R_position),S_position=as.numeric(S_position),T_position=as.numeric(T_position),U_position=as.numeric(U_position),
 strata1_CATE = as.numeric(strata1_CATE),strata1_LCL = as.numeric(strata1_LCL),strata1_UCL = as.numeric(strata1_UCL),
 strata2_CATE = as.numeric(strata2_CATE),strata2_LCL = as.numeric(strata2_LCL),strata2_UCL = as.numeric(strata2_UCL),
 strata3_CATE = as.numeric(strata3_CATE),strata3_LCL = as.numeric(strata3_LCL),strata3_UCL = as.numeric(strata3_UCL),
 strata4_CATE = as.numeric(strata4_CATE),strata4_LCL = as.numeric(strata4_LCL),strata4_UCL = as.numeric(strata4_UCL)) %>% 
 summarise_at(vars(B_position:strata4_UCL), mean)) %>% mutate(across(1:20, round, 3),across(21:32, round, 4))) 

# Transform table from wide to long
pivoted_table = pivot_longer(summarize, cols = everything(), values_to = "Average_Value",names_to = "Variable")  %>% mutate(Average_Value = as.numeric(Average_Value))
# Order the table by "Average_Value"
ordered_table1 = pivoted_table %>% filter(substr(Variable,3,10) == "position")
ordered_table2 = ordered_table1[order(ordered_table1$Average_Value),]
cates_table = pivoted_table %>% filter(substr(Variable,3,10) != "position")
summary_table = rbind(ordered_table2, cates_table)

 # Write summarized results to CSV in your working directory
  summary_table %>%   write.csv(paste0(".//summarize_",combo,"_",ortho,".csv"), row.names = FALSE)
  # Save the data frame with a name that we'll use for dataframe
  assign(paste0("summarize_",combo,"_",ortho), data.frame(summary_table))
    
# Clear all objects from R except for the final summarize and results tables
rm(list=grep("summarize|sim_result",ls(),value=TRUE,invert=TRUE))

# Return relevant data frames
if(exists("sim_results_13_1")) {
   sim_results_13_1 <<- sim_results_13_1
   summarize_13_1<<- summarize_13_1
}

if(exists("sim_results_13_0")) {
   sim_results_13_0 <<- sim_results_13_0
   summarize_13_0<<- summarize_13_0
}

if(exists("sim_results_14_1")) {
   sim_results_14_1 <<- sim_results_14_1
   summarize_14_1<<- summarize_14_1
}

if(exists("sim_results_14_0")) {
   sim_results_14_0 <<- sim_results_14_0
   summarize_14_0<<- summarize_14_0
}

if(exists("sim_results_15_1")) {
   sim_results_15_1 <<- sim_results_15_1
   summarize_15_1<<- summarize_15_1
}

if(exists("sim_results_15_0")) {
   sim_results_15_0 <<- sim_results_15_0
   summarize_15_0<<- summarize_15_0
}

if(exists("sim_results_16_1")) {
   sim_results_16_1 <<- sim_results_16_1
   summarize_16_1<<- summarize_16_1
}

if(exists("sim_results_16_0")) {
   sim_results_16_0 <<- sim_results_16_0
   summarize_16_0<<- summarize_16_0
}

if(exists("sim_results_17_1")) {
   sim_results_17_1 <<- sim_results_17_1
   summarize_17_1<<- summarize_17_1
}

if(exists("sim_results_17_0")) {
   sim_results_17_0 <<- sim_results_17_0
   summarize_17_0<<- summarize_17_0
}

if(exists("sim_results_18_1")) {
   sim_results_18_1 <<- sim_results_18_1
   summarize_18_1<<- summarize_18_1
}

if(exists("sim_results_18_0")) {
   sim_results_18_0 <<- sim_results_18_0
   summarize_18_0<<- summarize_18_0
}

if(exists("sim_results_19_1")) {
   sim_results_19_1 <<- sim_results_19_1
   summarize_19_1<<- summarize_19_1
}

if(exists("sim_results_19_0")) {
   sim_results_19_0 <<- sim_results_19_0
   summarize_19_0<<- summarize_19_0
}

if(exists("sim_results_20_1")) {
   sim_results_20_1 <<- sim_results_20_1
   summarize_20_1<<- summarize_20_1
}

if(exists("sim_results_20_0")) {
   sim_results_20_0 <<- sim_results_20_0
   summarize_20_0<<- summarize_20_0
}

if(exists("sim_results_21_1")) {
   sim_results_21_1 <<- sim_results_21_1
   summarize_21_1<<- summarize_21_1
}

if(exists("sim_results_21_0")) {
   sim_results_21_0 <<- sim_results_21_0
   summarize_21_0<<- summarize_21_0
}

if(exists("sim_results_22_1")) {
   sim_results_22_1 <<- sim_results_22_1
   summarize_22_1<<- summarize_22_1
}

if(exists("sim_results_22_0")) {
   sim_results_22_0 <<- sim_results_22_0
   summarize_22_0<<- summarize_22_0
}

if(exists("sim_results_23_1")) {
   sim_results_23_1 <<- sim_results_23_1
   summarize_23_1<<- summarize_23_1
}

if(exists("sim_results_23_0")) {
   sim_results_23_0 <<- sim_results_23_0
   summarize_23_0<<- summarize_23_0
}

if(exists("sim_results_24_1")) {
   sim_results_24_1 <<- sim_results_24_1
   summarize_24_1<<- summarize_24_1
}

if(exists("sim_results_24_0")) {
   sim_results_24_0 <<- sim_results_24_0
   summarize_24_0<<- summarize_24_0
}


} # end of the sim_cf_obs function
```






**Our sim_cf_obs function (above) which is tailored for an observational setting has now been defined.**
**Next, in Code Chunks 13-18, we run observational simulations using *high* correlations between variables and *small* heterogeneities in C.A.T.E.s.**

### Code Chunk 13: 13th group of simulations
```{r sim13, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# correlation can take the value of “high” or “low”, whereas cates can take the value of “small” or “large”
# combo can be any integer ranging from 13 to 24
# Run the 13th group of simulations, using 1000 individuals and 200 trees
sim_cf_obs(nIndividuals = 1000, nTrees = 200, correlation = "high", cates = "small", nSims = 1000, combo = 13, ortho = 1)
set.seed(930) 
sim_cf_obs(nIndividuals = 1000, nTrees = 200, correlation = "high", cates = "small", nSims = 1000, combo = 13, ortho = 0)         
```

### Code Chunk 14: 14th group of simulations
```{r sim14, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 14th group of simulations, using 1000 individuals and 2000 trees
sim_cf_obs(nIndividuals = 1000, nTrees = 2000, correlation = "high", cates = "small", nSims = 1000, combo = 14, ortho = 1)
set.seed(930) 
sim_cf_obs(nIndividuals = 1000, nTrees = 2000, correlation = "high", cates = "small", nSims = 1000, combo = 14, ortho = 0)


```

### Code Chunk 15: 15th group of simulations 
```{r sim15, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 15th group of simulations, using 10000 individuals and 200 trees
sim_cf_obs(nIndividuals = 10000, nTrees = 200, correlation = "high", cates = "small", nSims = 1000, combo = 15, ortho = 1)
set.seed(930) 
sim_cf_obs(nIndividuals = 10000, nTrees = 200, correlation = "high", cates = "small", nSims = 1000, combo = 15, ortho = 0)

```

### Code Chunk 16: 16th group of simulations 
```{r sim16, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 16th group of simulations, using 10000 individuals and 2000 trees
sim_cf_obs(nIndividuals = 10000, nTrees = 2000, correlation = "high", cates = "small", nSims = 1000, combo = 16, ortho = 1)
set.seed(930) 
sim_cf_obs(nIndividuals = 10000, nTrees = 2000, correlation = "high", cates = "small", nSims = 1000, combo = 16, ortho = 0)
```

### Code Chunk 17: 17th group of simulations 
```{r sim17, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 17th group of simulations, using 40000 individuals and 200 trees
sim_cf_obs(nIndividuals = 40000, nTrees = 200, correlation = "high", cates = "small", nSims = 1000, combo = 17, ortho = 1)
set.seed(930) 
sim_cf_obs(nIndividuals = 40000, nTrees = 200, correlation = "high", cates = "small", nSims = 1000, combo = 17, ortho = 0)

```

### Code Chunk 18: 18th group of simulations 
```{r sim18, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 18th group of simulations, using 40000 individuals and 2000 trees
sim_cf_obs(nIndividuals = 40000, nTrees = 2000, correlation = "high", cates = "small", nSims = 1000, combo = 18, ortho = 1)
set.seed(930) 
sim_cf_obs(nIndividuals = 40000, nTrees = 2000, correlation = "high", cates = "small", nSims = 1000, combo = 18, ortho = 0)

```
**In Code Chunks 19-24, we run simulations using *low* correlations between variables and *large* heterogeneities in C.A.T.E.s.**


### Code Chunk 19: 19th group of simulations
```{r sim19, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 19th group of simulations, using 1000 individuals and 200 trees
sim_cf_obs(nIndividuals = 1000, nTrees = 200, correlation = "low", cates = "large", nSims = 1000, combo = 19, ortho = 1)
set.seed(930) 
sim_cf_obs(nIndividuals = 1000, nTrees = 200, correlation = "low", cates = "large", nSims = 1000, combo = 19, ortho = 0)
```

### Code Chunk 20: 20st group of simulations
```{r sim20, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 20st group of simulations, using 1000 individuals and 2000 trees
sim_cf_obs(nIndividuals = 1000, nTrees = 2000, correlation = "low", cates = "large", nSims = 1000, combo = 20, ortho = 1)
set.seed(930) 
sim_cf_obs(nIndividuals = 1000, nTrees = 2000, correlation = "low", cates = "large", nSims = 1000, combo = 20, ortho = 0)
```

### Code Chunk 21: 21st group of simulations 
```{r sim21, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 21st group of simulations, using 10000 individuals and 200 trees
sim_cf_obs(nIndividuals = 10000, nTrees = 200, correlation = "low", cates = "large", nSims = 1000, combo = 21, ortho = 1)
set.seed(930) 
sim_cf_obs(nIndividuals = 10000, nTrees = 200, correlation = "low", cates = "large", nSims = 1000, combo = 21, ortho = 0)
```

### Code Chunk 22: 22nd group of simulations 
```{r sim22, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 22nd group of simulations, using 10000 individuals and 2000 trees
sim_cf_obs(nIndividuals = 10000, nTrees = 2000, correlation = "low", cates = "large",  nSims = 1000, combo = 22, ortho = 1)
set.seed(930) 
sim_cf_obs(nIndividuals = 10000, nTrees = 2000, correlation = "low", cates = "large",  nSims = 1000, combo = 22, ortho = 0)
```

### Code Chunk 23: 23rd group of simulations 
```{r sim23, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 23rd group of simulations, using 40000 individuals and 200 trees
sim_cf_obs(nIndividuals = 40000, nTrees = 200, correlation = "low", cates = "large",  nSims = 1000, combo = 23, ortho = 1)
set.seed(930) 
sim_cf_obs(nIndividuals = 40000, nTrees = 200, correlation = "low", cates = "large",  nSims = 1000, combo = 23, ortho = 0)
```

### Code Chunk 24: 24th group of simulations 
```{r sim24, include=TRUE}
# Note: check working directory before running. This is where the results files will get output
getwd() 
# Load packages
library("dplyr") # dplyr (Version 1.0.9) - load data management package
library("tidyr") # tidyr (Version 1.2.0) - load data management package
library("faux") # faux (Version 1.0.0) - load package for generating correlated variables
library("grf")  # grf (Version 2.0.2) - load package for honest causal forest
library("estimatr")  # estimatr (Version 0.30.6) - library for lm_robust
library("kableExtra")  # kableExtra (Version 1.3.4) - library for cell_spec
library("knitr")  # knitr (Version 1.39) - library for kable table
webshot::install_phantomjs()  # PhantomJS (Version 2.1.1) - library for kable table
# Important: Prior to running each group of simulations, one should always set the seed. This helps to ensure that the values generated in each variable are reproducible.
set.seed(930) 
# Run the 24th group of simulations, using 40000 individuals and 2000 trees
sim_cf_obs(nIndividuals = 40000, nTrees = 2000, correlation = "low", cates = "large",  nSims = 1000, combo = 24, ortho = 1)
set.seed(930) 
sim_cf_obs(nIndividuals = 40000, nTrees = 2000, correlation = "low", cates = "large",  nSims = 1000, combo = 24, ortho = 0)

```


